{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dungeons and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is a first in a series of posts of applying machine learning to dungeons and dragons. This blog post details my findings with applying reinforcement learning algorithms in a simple [Dungeons and Dragons combat](https://www.youtube.com/watch?v=7tnrATiclg4) scenario. Future blog posts will include:\n",
    "\n",
    " * Applying RL algorithms to more complicated combat scenarios\n",
    " * Applying NLP to the D&D story telling aspect\n",
    " \n",
    "In the spirit of full disclosure, this blog post was mainly made because I could not definitively figure out why my DQN agent failed to learn a reasonable strategy (details below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combat Scenario Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section discusses the environment in which combat takes place. \n",
    "\n",
    "In order to first allow for learning in a simple environment, the combat takes place in a 50ft x 50ft room and only involves two combatants:\n",
    "\n",
    "1. Leotris:\n",
    "    * Hit points: 25\n",
    "    * Armor class: 16\n",
    "    * Speed: 30ft\n",
    "    * Shoot arrow attack: \n",
    "        * 60 ft range\n",
    "        * Hit bonus: +5\n",
    "        * Damage dice: 1d12\n",
    "        * Damage bonus: +3\n",
    "    * Initial coordinates: (5, 5)\n",
    "\n",
    "\n",
    "2. Strahd:\n",
    "    * Hit points: 200\n",
    "    * Armor class: 16\n",
    "    * Speed: 30ft\n",
    "    * Vampire bite attack:\n",
    "        * 5 ft range\n",
    "        * Hit bonus: +10\n",
    "        * Damage dice: 3d12\n",
    "        * Damage bonus: +10\n",
    "    * Initial coordinates: (5, 10)\n",
    "    \n",
    "Each combatant, along with the attacks listed above, is allowed the following actions:\n",
    "    * MoveLeft: move left 5ft\n",
    "    * MoveRight: move right 5ft\n",
    "    * MoveDown: move down 5ft\n",
    "    * MoveUp: move up 5ft\n",
    "    * EndTurn: ends turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first experiment, `Leotris` was assigned one of the RL algorithms described below while `Strahd` was assigned a `Random` strategy in which actions were chosen at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling Double Deep Q-Learning Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Iteration (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation of [PPO](https://arxiv.org/abs/1707.06347) managed to obtain the best results:\n",
    "\n",
    "![](results/PPO.png)\n",
    "\n",
    "I believe that there were a couple contributing factors in obtaining these results. First, the learning rate had to be sufficiently small (alpha = 1e-5). With a larger learning rate of (alpha 1e-3), the following results were observed:\n",
    "\n",
    "![](results/PPO_high_alpha.png)\n",
    "\n",
    "In the case depicted above, the agent seems to have gotten into a parameter space in which it could not recover from\n",
    "\n",
    "Second, the agent did not use an epsilon greedy like exploration strategy. Intead, PPO selects actions in a more stochastic nature compared to the epsilon greedy approach once its reached low exploration states. As a result, this avoided the problem observed when using variants of Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
