{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dungeons and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is a first in a series of posts of applying machine learning to dungeons and dragons. This blog post details my findings with applying reinforcement learning algorithms in a simple [Dungeons and Dragons combat](https://www.youtube.com/watch?v=7tnrATiclg4) scenario. Future blog posts will include:\n",
    "\n",
    " * Applying RL algorithms to more complicated combat scenarios\n",
    " * Applying NLP to the story telling aspect of D&D \n",
    " \n",
    "In the spirit of full disclosure, this blog post was mainly made because I could not definitively figure out why my DQN agent failed to learn a reasonable strategy (details below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combat Scenario Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section discusses the environment in which combat takes place. \n",
    "\n",
    "In order to first allow for learning in a simple environment, the combat takes place in a 50ft x 50ft room and only involves two combatants:\n",
    "\n",
    "1. Leotris:\n",
    "    * Hit points: 25\n",
    "    * Armor class: 16\n",
    "    * Speed: 30ft\n",
    "    * Shoot arrow attack: \n",
    "        * 60 ft range\n",
    "        * Hit bonus: +5\n",
    "        * Damage dice: 1d12\n",
    "        * Damage bonus: +3\n",
    "    * Initial coordinates: (5, 5)\n",
    "\n",
    "\n",
    "2. Strahd:\n",
    "    * Hit points: 200\n",
    "    * Armor class: 16\n",
    "    * Speed: 30ft\n",
    "    * Vampire bite attack:\n",
    "        * 5 ft range\n",
    "        * Hit bonus: +10\n",
    "        * Damage dice: 3d12\n",
    "        * Damage bonus: +10\n",
    "    * Initial coordinates: (5, 10)\n",
    "    \n",
    "Each combatant, along with the attacks listed above, is allowed the following actions:\n",
    "    * MoveLeft: move left 5ft\n",
    "    * MoveRight: move right 5ft\n",
    "    * MoveDown: move down 5ft\n",
    "    * MoveUp: move up 5ft\n",
    "    * EndTurn: ends turn\n",
    "    \n",
    "Additionally, a \"time limit\" of 1500 actions was implemented. In other words, agents were permitted to take a maximum of 1500 actions within a single combat encounter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals and Learning Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, `Leotris` was assigned one of the RL algorithms described below while `Strahd` was assigned a `Random` strategy in which actions were chosen at random.\n",
    "\n",
    "The scenario was purposefully set up such that Strahd had an obvious advantage, but Leotris may be able learn to keep his distance from Strahd and use his `ShootArrow` attack in order to win. \n",
    "\n",
    "The following are learning goals I envisioned for this project:\n",
    "\n",
    "* `Leotris` out performs strategy of taking random actions.\n",
    "* `Leotris` learns to dealt damage. The challenge with this goal is that the agent must learn that it cannot just repeatedly take the `ShootArrow` action within the same turn. Instead, due to D&D combat rules, the agent must take the `EndTurn` action in between each `ShootArrow` usage if damage is to be done.\n",
    "* `Leotris` learns to avoid damage. `Leotris` can only take damage from `Strahd` if they are within 5 ft of each other. \n",
    "\n",
    "In order to accomplish the above goals, the agent's \"state\" consists of the following:\n",
    "\n",
    "1. Current hit points\n",
    "2. Enemy hit points\n",
    "3. Current x-coordinate\n",
    "4. Current y-coordinate\n",
    "5. Enemy x-coordinate\n",
    "6. Enemy y-coordinate\n",
    "7. Number of attacks used this turn\n",
    "8. Remaining movement available this turn\n",
    "9. Time limit remaining (as a percentage of the time limit)\n",
    "\n",
    "A reward of `5` was given if `Leotris` was the winner. Otherwise, a reward of `0` was given if `Leotris` had taken enough actions to reach the \"time limit\" (1500 actions) or his hit points were reduced to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random\n",
    "\n",
    "Below are the result of both `Strahd` and `Leotris` using actions at random.\n",
    "\n",
    "![](results/random.png)\n",
    "\n",
    "The above was used to evaluate whether goal 1 had been achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling Double Deep Q-Learning Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a dueling double deep Q network, the follow results were achieved:\n",
    "\n",
    "![dddqn_results](results/double_dueling_DQN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above shows a failure to learn a reasonable strategy. Similar results were observed with vanilla DQN and double DQN. \n",
    "\n",
    "I believe the largest contributing factor is the use of a linear decay $\\epsilon$-greedy exploration strategy. Specfically, the strategy employed to achieve the results above started with an $\\epsilon$ exploration probability of 90%. That is to say, 10% of the time, the agent would take the action believed to be optimal, and the remaining 90% of the time, the agent would \"explore\" by taking a random action. The $\\epsilon$ value would decay linearly down to a 5% probability of taking a random action. The agent seemed to learn that the action `ShootArrow` was the best action to perform regardless of the `current_state`. \n",
    "\n",
    "As a consequence, the agent would attempt to `ShootArrow` despite the fact that it had already used the action earlier within the same turn. As a result, the action would be ignored and the agent would be prompted for its next action until the `EndTurn` action was chosen or the time limit was reached. The agent never learned when to take the `EndTurn` action consistently resulted a `Timeout` terminal state.\n",
    "\n",
    "Here are some additional hypothesis about why this problem was happening and what I tried to do to correct for it.\n",
    "\n",
    "1. **$\\epsilon$ decay rate was too fast**: Perhaps the neural network did not have enough time/epochs to learn that taking the `EndTurn` action would be beneficial. I decreased the decay rate by a factor of 100 but saw not significant changes. \n",
    "2. **Sparse rewards**: Perhaps only providing a reward for achieving a victory was too sparse. To address this, I structured the rewards such that the agent was rewarded every time it was able to do damage. This helped a great deal with the agent even learning to alter between `ShootArrow` and `EndTurn`. However, when the agent was left to run for a long enough time, eventually, the `TimeOut` terminal state was take over. \n",
    "![damage_reward_results](results/damage_reward_dddqn.png)\n",
    "\n",
    "3. **Off-policy learning**: DQNs optimize for the case where the $\\epsilon$-greedy strategy is replaced by one that is always taking the optimal action without exploration. This is a result of DQNs being off policy. Intuitively, I thought that this rectify the problem of `ShootArrow` being the only action chosen. This was because the off-policy learning would optimize for a case where the agent would not depend on `EndTurn` randomly being chosen. Despite this intuition, and full of desperation, I implemented SARSA as a quick test for on policy learning. Results were not improved. \n",
    "4. **Catastrophic forgetting**: Looking at [this](https://ai.stackexchange.com/questions/10822/what-is-happening-when-a-reinforcement-learning-agent-trains-itself-out-of-desir) stack exchange post, the user is asking why DQNs sometimes train themselves out of desired behavior. This was observed when the rewards were altered to reward any damage being delt. One answer given suggested that this could be a result of \"catastrophic forgetting\" (detailed in the post). To combat this, I tried to decrease the learning rate and use prioritized experience replay. Both of these did not work. Another suggestion was to keep experiences from early stages of exploration within memory. I have not tried this one yet. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Iteration (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation of [PPO](https://arxiv.org/abs/1707.06347) managed to obtain the best results:\n",
    "\n",
    "![](results/PPO.png)\n",
    "\n",
    "I believe that there were a couple contributing factors in obtaining these results. First, the learning rate had to be sufficiently small (alpha = 1e-5). With a larger learning rate of (alpha 1e-3), the following results were observed:\n",
    "\n",
    "![](results/PPO_high_alpha.png)\n",
    "\n",
    "In the case depicted above, the agent seems to have gotten into a parameter space in which it could not recover from\n",
    "\n",
    "Second, the agent did not use an epsilon greedy like exploration strategy. Intead, PPO selects actions in a more stochastic nature compared to the epsilon greedy approach once its reached low exploration states. As a result, this avoided the problem observed when using variants of Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
